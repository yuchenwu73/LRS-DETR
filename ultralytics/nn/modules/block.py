# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""Block modules."""

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

from .conv import Conv, DWConv, GhostConv, LightConv, RepConv
from .transformer import TransformerBlock

__all__ = ('DFL', 'HGBlock', 'HGStem', 'SPP', 'SPPF', 'C1', 'C2', 'C3', 'CSPLayer', 'C3x', 'C3TR', 'C3Ghost',
           'GhostBottleneck', 'Bottleneck', 'BottleneckCSP', 'Proto', 'RepC3', 'ConvNormLayer', 'BasicBlock', 
           'BottleNeck', 'Blocks')


class DFL(nn.Module):
    """
    Integral module of Distribution Focal Loss (DFL).

    Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391
    """

    def __init__(self, c1=16):
        """Initialize a convolutional layer with a given number of input channels."""
        super().__init__()
        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)
        x = torch.arange(c1, dtype=torch.float)
        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
        self.c1 = c1

    def forward(self, x):
        """Applies a transformer layer on input tensor 'x' and returns a tensor."""
        b, c, a = x.shape  # batch, channels, anchors
        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
        # return self.conv(x.view(b, self.c1, 4, a).softmax(1)).view(b, 4, a)


class Proto(nn.Module):
    """YOLOv8 mask Proto module for segmentation models."""

    def __init__(self, c1, c_=256, c2=32):
        """
        Initializes the YOLOv8 mask Proto module with specified number of protos and masks.

        Input arguments are ch_in, number of protos, number of masks.
        """
        super().__init__()
        self.cv1 = Conv(c1, c_, k=3)
        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')
        self.cv2 = Conv(c_, c_, k=3)
        self.cv3 = Conv(c_, c2)

    def forward(self, x):
        """Performs a forward pass through layers using an upsampled input image."""
        return self.cv3(self.cv2(self.upsample(self.cv1(x))))


class HGStem(nn.Module):
    """
    StemBlock of PPHGNetV2 with 5 convolutions and one maxpool2d.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2):
        """Initialize the SPP layer with input/output channels and specified kernel sizes for max pooling."""
        super().__init__()
        self.stem1 = Conv(c1, cm, 3, 2, act=nn.ReLU())
        self.stem2a = Conv(cm, cm // 2, 2, 1, 0, act=nn.ReLU())
        self.stem2b = Conv(cm // 2, cm, 2, 1, 0, act=nn.ReLU())
        self.stem3 = Conv(cm * 2, cm, 3, 2, act=nn.ReLU())
        self.stem4 = Conv(cm, c2, 1, 1, act=nn.ReLU())
        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, ceil_mode=True)

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        x = self.stem1(x)
        x = F.pad(x, [0, 1, 0, 1])
        x2 = self.stem2a(x)
        x2 = F.pad(x2, [0, 1, 0, 1])
        x2 = self.stem2b(x2)
        x1 = self.pool(x)
        x = torch.cat([x1, x2], dim=1)
        x = self.stem3(x)
        x = self.stem4(x)
        return x


class HGBlock(nn.Module):
    """
    HG_Block of PPHGNetV2 with 2 convolutions and LightConv.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2, k=3, n=6, lightconv=False, shortcut=False, act=nn.ReLU()):
        """Initializes a CSP Bottleneck with 1 convolution using specified input and output channels."""
        super().__init__()
        block = LightConv if lightconv else Conv
        self.m = nn.ModuleList(block(c1 if i == 0 else cm, cm, k=k, act=act) for i in range(n))
        self.sc = Conv(c1 + n * cm, c2 // 2, 1, 1, act=act)  # squeeze conv
        self.ec = Conv(c2 // 2, c2, 1, 1, act=act)  # excitation conv
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        y = [x]
        y.extend(m(y[-1]) for m in self.m)
        y = self.ec(self.sc(torch.cat(y, 1)))
        return y + x if self.add else y


class SPP(nn.Module):
    """Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729."""

    def __init__(self, c1, c2, k=(5, 9, 13)):
        """Initialize the SPP layer with input/output channels and pooling kernel sizes."""
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        """Forward pass of the SPP layer, performing spatial pyramid pooling."""
        x = self.cv1(x)
        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))


class SPPF(nn.Module):
    """Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher."""

    def __init__(self, c1, c2, k=5):
        """
        Initializes the SPPF layer with given input/output channels and kernel size.

        This module is equivalent to SPP(k=(5, 9, 13)).
        """
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        """Forward pass through Ghost Convolution block."""
        x = self.cv1(x)
        y1 = self.m(x)
        y2 = self.m(y1)
        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))


class C1(nn.Module):
    """CSP Bottleneck with 1 convolution."""

    def __init__(self, c1, c2, n=1):
        """Initializes the CSP Bottleneck with configurations for 1 convolution with arguments ch_in, ch_out, number."""
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.m = nn.Sequential(*(Conv(c2, c2, 3) for _ in range(n)))

    def forward(self, x):
        """Applies cross-convolutions to input in the C3 module."""
        y = self.cv1(x)
        return self.m(y) + y


class C2(nn.Module):
    """CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initializes the CSP Bottleneck with 2 convolutions module with arguments ch_in, ch_out, number, shortcut,
        groups, expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv(2 * self.c, c2, 1)  # optional act=FReLU(c2)
        # self.attention = ChannelAttention(2 * self.c)  # or SpatialAttention()
        self.m = nn.Sequential(*(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        a, b = self.cv1(x).chunk(2, 1)
        return self.cv2(torch.cat((self.m(a), b), 1))


class CSPLayer(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        """Forward pass through CSPLayer layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))


class C3(nn.Module):
    """CSP Bottleneck with 3 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


class C3x(C3):
    """C3 module with cross-convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3TR instance and set default parameters."""
        super().__init__(c1, c2, n, shortcut, g, e)
        self.c_ = int(c2 * e)
        self.m = nn.Sequential(*(Bottleneck(self.c_, self.c_, shortcut, g, k=((1, 3), (3, 1)), e=1) for _ in range(n)))


class RepC3(nn.Module):
    """Rep C3."""

    def __init__(self, c1, c2, n=3, e=1.0):
        """Initialize CSP Bottleneck with a single convolution using input channels, output channels, and number."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])
        self.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()

    def forward(self, x):
        """Forward pass of RT-DETR neck layer."""
        return self.cv3(self.m(self.cv1(x)) + self.cv2(x))


class C3TR(C3):
    """C3 module with TransformerBlock()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3Ghost module with GhostBottleneck()."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)


class C3Ghost(C3):
    """C3 module with GhostBottleneck()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))


class GhostBottleneck(nn.Module):
    """Ghost Bottleneck https://github.com/huawei-noah/ghostnet."""

    def __init__(self, c1, c2, k=3, s=1):
        """Initializes GhostBottleneck module with arguments ch_in, ch_out, kernel, stride."""
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # pw
            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
            GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1,
                                                                            act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        """Applies skip connection and concatenation to input tensor."""
        return self.conv(x) + self.shortcut(x)


class Bottleneck(nn.Module):
    """Standard bottleneck."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
        expansion.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """'forward()' applies the YOLO FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


class BottleneckCSP(nn.Module):
    """CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initializes the CSP Bottleneck given arguments for ch_in, ch_out, number, shortcut, groups, expansion."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        """Applies a CSP bottleneck with 3 convolutions."""
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))

################################### RT-DETR PResnet ###################################
"""
RT-DETR PResnetç›¸å…³æ¨¡å—
è¿™éƒ¨åˆ†ä»£ç å®šä¹‰äº†ç”¨äºæ„å»ºç±»ä¼¼ResNetçš„ä¸»å¹²ç½‘ç»œçš„ä¸€äº›åŸºç¡€æ¨¡å—ï¼Œ
ç‰¹åˆ«æ˜¯ConvNormLayerã€BasicBlockå’ŒBottleNeckã€‚
"""

def get_activation(act: str, inpace: bool=True):
    """
    æ ¹æ®å­—ç¬¦ä¸²åç§°è·å–æ¿€æ´»å‡½æ•°å®ä¾‹ã€‚
    è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ–¹ä¾¿åœ°é€šè¿‡å­—ç¬¦ä¸²æŒ‡å®šå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚
    
    å‚æ•°:
        act (str): æ¿€æ´»å‡½æ•°çš„åç§° (ä¾‹å¦‚, 'silu', 'relu', 'gelu')ã€‚
        inpace (bool): æ˜¯å¦ä½¿ç”¨inplaceæ“ä½œï¼ˆå¦‚æœæ¿€æ´»å‡½æ•°æ”¯æŒï¼‰ã€‚é»˜è®¤ä¸ºTrueã€‚
        
    è¿”å›:
        nn.Module: å¯¹åº”çš„æ¿€æ´»å‡½æ•°æ¨¡å—å®ä¾‹ã€‚
        
    å¼‚å¸¸:
        RuntimeError: å¦‚æœæä¾›çš„æ¿€æ´»å‡½æ•°åç§°ä¸æ”¯æŒã€‚
    """
    # å°†æ¿€æ´»å‡½æ•°åç§°è½¬æ¢ä¸ºå°å†™ï¼Œä»¥ä¾¿ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
    act = act.lower()
    
    # æ ¹æ®åç§°è¿”å›å¯¹åº”çš„PyTorchæ¿€æ´»å‡½æ•°æ¨¡å—
    if act == 'silu':
        m = nn.SiLU()
    elif act == 'relu':
        m = nn.ReLU()
    elif act == 'leaky_relu':
        m = nn.LeakyReLU()
    # æ³¨æ„ï¼šè¿™é‡Œæœ‰ä¸€ä¸ªé‡å¤çš„'silu'æ£€æŸ¥ï¼Œå¯èƒ½æ˜¯ç¬”è¯¯ï¼Œä½†ä¿æŒåŸæ ·
    elif act == 'silu':
        m = nn.SiLU()
    elif act == 'gelu':
        m = nn.GELU()
    # å¦‚æœactä¸ºNoneï¼Œåˆ™è¿”å›ä¸€ä¸ªæ’ç­‰æ˜ å°„ï¼Œå³ä¸åº”ç”¨æ¿€æ´»å‡½æ•°
    elif act is None:
        m = nn.Identity()
    # å¦‚æœactæœ¬èº«å°±æ˜¯ä¸€ä¸ªnn.Moduleå®ä¾‹ï¼Œåˆ™ç›´æ¥è¿”å›å®ƒ
    elif isinstance(act, nn.Module):
        m = act
    # å¦‚æœä»¥ä¸Šæ¡ä»¶éƒ½ä¸æ»¡è¶³ï¼Œåˆ™æŠ›å‡ºè¿è¡Œæ—¶é”™è¯¯
    else:
        raise RuntimeError(f'ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°ç±»å‹: {act}')  

    # æ£€æŸ¥è¿”å›çš„æ¨¡å—æ˜¯å¦æœ‰inplaceå±æ€§ï¼Œå¦‚æœæœ‰å¹¶ä¸”inpaceå‚æ•°ä¸ºTrueï¼Œåˆ™è®¾ç½®å®ƒ
    if hasattr(m, 'inplace'):
        m.inplace = inpace
    
    return m 

class ConvNormLayer(nn.Module):
    """
    å·ç§¯-å½’ä¸€åŒ–å±‚
    
    å°†å·ç§¯å±‚ã€æ‰¹å½’ä¸€åŒ–å±‚å’Œå¯é€‰çš„æ¿€æ´»å‡½æ•°ç»„åˆåœ¨ä¸€èµ·çš„æ ‡å‡†æ¨¡å—ã€‚
    è¿™æ˜¯æ„å»ºæ·±åº¦å·ç§¯ç½‘ç»œçš„åŸºæœ¬ç»„ä»¶ã€‚
    """
    def __init__(self, ch_in, ch_out, kernel_size, stride, padding=None, bias=False, act=None):
        """
        åˆå§‹åŒ–ConvNormLayer
        
        å‚æ•°:
            ch_in (int): è¾“å…¥é€šé“æ•°ã€‚
            ch_out (int): è¾“å‡ºé€šé“æ•°ã€‚
            kernel_size (int): å·ç§¯æ ¸å¤§å°ã€‚
            stride (int): å·ç§¯æ­¥é•¿ã€‚
            padding (int, optional): å¡«å……å¤§å°ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨è®¡ç®—ä»¥ä¿æŒç©ºé—´ç»´åº¦ã€‚
            bias (bool): å·ç§¯å±‚æ˜¯å¦ä½¿ç”¨åç½®ã€‚é»˜è®¤ä¸ºFalseï¼Œå› ä¸ºæ‰¹å½’ä¸€åŒ–å±‚åŒ…å«äº†åç½®ã€‚
            act (str or nn.Module, optional): æ¿€æ´»å‡½æ•°çš„åç§°æˆ–å®ä¾‹ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ã€‚
        """
        super().__init__()
        # å®šä¹‰å·ç§¯å±‚
        self.conv = nn.Conv2d(
            ch_in, 
            ch_out, 
            kernel_size, 
            stride, 
            # å¦‚æœpaddingä¸ºNoneï¼Œåˆ™è‡ªåŠ¨è®¡ç®—å¡«å……å¤§å°ä»¥ä½¿è¾“å‡ºå°ºå¯¸åœ¨stride=1æ—¶ä¿æŒä¸å˜
            padding=(kernel_size-1)//2 if padding is None else padding, 
            bias=bias)
        # å®šä¹‰æ‰¹å½’ä¸€åŒ–å±‚
        self.norm = nn.BatchNorm2d(ch_out)
        # è·å–æ¿€æ´»å‡½æ•°å±‚ï¼Œå¦‚æœactä¸ºNoneåˆ™ä½¿ç”¨æ’ç­‰æ˜ å°„
        self.act = nn.Identity() if act is None else get_activation(act) 

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ï¼šConv -> BN -> Act
        """
        return self.act(self.norm(self.conv(x)))
    
    def forward_fuse(self, x):
        """
        ç”¨äºèåˆæ¨ç†çš„å‰å‘ä¼ æ’­å‡½æ•°ï¼šConv -> Act
        
        åœ¨æ¨¡å‹éƒ¨ç½²æˆ–æ¨ç†æ—¶ï¼Œå¯ä»¥å°†å·ç§¯å±‚å’Œæ‰¹å½’ä¸€åŒ–å±‚èåˆæˆä¸€ä¸ªå•ç‹¬çš„å·ç§¯å±‚ï¼Œ
        è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿäº†èåˆåçš„è¡Œä¸ºï¼ˆå‡è®¾BNå±‚å·²è¢«èåˆè¿›Convå±‚ï¼‰ã€‚
        """
        # """æ‰§è¡ŒäºŒç»´æ•°æ®çš„è½¬ç½®å·ç§¯ã€‚""" (åŸè‹±æ–‡æ³¨é‡Šç¿»è¯‘ï¼Œä½†ä¼¼ä¹ä¸ä»£ç åŠŸèƒ½ä¸ç¬¦ï¼Œä»£ç å®é™…æ˜¯ Conv -> Act)
        return self.act(self.conv(x))

class BasicBlock(nn.Module):
    """
    ResNetåŸºç¡€æ®‹å·®å—
    
    åŒ…å«ä¸¤ä¸ª3x3å·ç§¯å±‚å’Œä¸€ä¸ªå¯é€‰çš„shortcutè¿æ¥ã€‚
    é€šé“æ‰©å±•ç³»æ•°ä¸º1ã€‚
    """
    # å®šä¹‰é€šé“æ‰©å±•ç³»æ•°ï¼ŒBasicBlockä¸æ”¹å˜é€šé“æ•°ï¼ˆç›¸å¯¹äºch_outï¼‰
    expansion = 1

    def __init__(self, ch_in, ch_out, stride, shortcut, act='relu', variant='d'):
        """
        åˆå§‹åŒ–BasicBlock
        
        å‚æ•°:
            ch_in (int): è¾“å…¥é€šé“æ•°ã€‚
            ch_out (int): è¾“å‡ºé€šé“æ•°ã€‚
            stride (int): ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„æ­¥é•¿ï¼Œç”¨äºä¸‹é‡‡æ ·ã€‚
            shortcut (bool): æ˜¯å¦ä½¿ç”¨shortcutè¿æ¥ï¼ˆè¾“å…¥ç›´æ¥æ·»åŠ åˆ°è¾“å‡ºï¼‰ã€‚
            act (str): æ¿€æ´»å‡½æ•°ç±»å‹ã€‚é»˜è®¤ä¸º'relu'ã€‚
            variant (str): ResNetå˜ä½“ç±»å‹ï¼Œå½±å“shortcutçš„å®ç°æ–¹å¼ã€‚é»˜è®¤ä¸º'd'ã€‚
        """
        super().__init__()

        self.shortcut = shortcut

        # å¦‚æœä¸ä½¿ç”¨æ’ç­‰shortcutï¼ˆä¾‹å¦‚ï¼Œå½“è¾“å…¥è¾“å‡ºé€šé“æ•°æˆ–ç©ºé—´ç»´åº¦ä¸åŒæ—¶ï¼‰
        if not shortcut:
            # ResNet-Då˜ä½“ï¼šå¦‚æœéœ€è¦ä¸‹é‡‡æ ·(stride=2)ï¼Œshortcutè·¯å¾„ä½¿ç”¨å¹³å‡æ± åŒ–+1x1å·ç§¯
            if variant == 'd' and stride == 2:
                self.short = nn.Sequential(OrderedDict([
                    ('pool', nn.AvgPool2d(2, 2, 0, ceil_mode=True)),
                    ('conv', ConvNormLayer(ch_in, ch_out, 1, 1))
                ]))
            # å…¶ä»–æƒ…å†µæˆ–éResNet-Då˜ä½“ï¼šshortcutè·¯å¾„ä½¿ç”¨æ­¥é•¿ä¸ºstrideçš„1x1å·ç§¯
            else:
                self.short = ConvNormLayer(ch_in, ch_out, 1, stride)

        # æ®‹å·®è·¯å¾„çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œæ­¥é•¿ä¸ºstride
        self.branch2a = ConvNormLayer(ch_in, ch_out, 3, stride, act=act)
        # æ®‹å·®è·¯å¾„çš„ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼Œæ­¥é•¿ä¸º1ï¼Œä¸å¸¦æ¿€æ´»å‡½æ•°ï¼ˆæ¿€æ´»å‡½æ•°åœ¨ä¸shortcutç›¸åŠ ååº”ç”¨ï¼‰
        self.branch2b = ConvNormLayer(ch_out, ch_out, 3, 1, act=None)
        # è·å–æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°
        self.act = nn.Identity() if act is None else get_activation(act) 


    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        """
        # è®¡ç®—æ®‹å·®è·¯å¾„çš„è¾“å‡º
        out = self.branch2a(x)
        out = self.branch2b(out)
        
        # è·å–shortcutè·¯å¾„çš„è¾“å‡º
        if self.shortcut:
            # å¦‚æœä½¿ç”¨æ’ç­‰shortcutï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥x
            short = x
        else:
            # å¦åˆ™ï¼Œé€šè¿‡å®šä¹‰çš„shortæ¨¡å—å¤„ç†è¾“å…¥x
            short = self.short(x)
        
        # å°†æ®‹å·®è·¯å¾„å’Œshortcutè·¯å¾„çš„è¾“å‡ºç›¸åŠ 
        out = out + short
        # åº”ç”¨æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°
        out = self.act(out)

        return out


class BottleNeck(nn.Module):
    """
    ResNetç“¶é¢ˆæ®‹å·®å—
    
    åŒ…å«ä¸€ä¸ª1x1å·ç§¯ï¼ˆé™ç»´ï¼‰ï¼Œä¸€ä¸ª3x3å·ç§¯å’Œä¸€ä¸ª1x1å·ç§¯ï¼ˆå‡ç»´ï¼‰ï¼Œä»¥åŠå¯é€‰çš„shortcutè¿æ¥ã€‚
    é€šé“æ‰©å±•ç³»æ•°é€šå¸¸ä¸º4ã€‚
    """
    # å®šä¹‰é€šé“æ‰©å±•ç³»æ•°ï¼Œç“¶é¢ˆå—çš„è¾“å‡ºé€šé“æ•°æ˜¯å†…éƒ¨é€šé“æ•°(ch_out)çš„4å€
    expansion = 4

    def __init__(self, ch_in, ch_out, stride, shortcut, act='relu', variant='d'):
        """
        åˆå§‹åŒ–BottleNeckå—
        
        å‚æ•°:
            ch_in (int): è¾“å…¥é€šé“æ•°ã€‚
            ch_out (int): å†…éƒ¨ï¼ˆéšè—å±‚ï¼‰é€šé“æ•°ã€‚æœ€ç»ˆè¾“å‡ºé€šé“æ•°ä¸º ch_out * expansionã€‚
            stride (int): æ­¥é•¿ï¼Œé€šå¸¸åº”ç”¨äºä¸­é—´çš„3x3å·ç§¯ï¼ˆå˜ä½“bï¼‰æˆ–ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼ˆå˜ä½“aï¼‰ã€‚
            shortcut (bool): æ˜¯å¦ä½¿ç”¨shortcutè¿æ¥ã€‚
            act (str): æ¿€æ´»å‡½æ•°ç±»å‹ã€‚é»˜è®¤ä¸º'relu'ã€‚
            variant (str): ResNetå˜ä½“ç±»å‹ã€‚'a'è¡¨ç¤ºæ­¥é•¿åœ¨ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼Œ'b'/'d'è¡¨ç¤ºæ­¥é•¿åœ¨3x3å·ç§¯ã€‚
        """
        super().__init__()

        # æ ¹æ®å˜ä½“ç±»å‹ç¡®å®šæ­¥é•¿åº”ç”¨åœ¨å“ªä¸€å±‚å·ç§¯
        if variant == 'a':
            # å˜ä½“a: æ­¥é•¿åœ¨ç¬¬ä¸€ä¸ª1x1å·ç§¯
            stride1, stride2 = stride, 1
        else:
            # å˜ä½“b/d: æ­¥é•¿åœ¨ä¸­é—´çš„3x3å·ç§¯
            stride1, stride2 = 1, stride

        # å†…éƒ¨é€šé“æ•°
        width = ch_out 

        # æ®‹å·®è·¯å¾„çš„ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼ˆé™ç»´ï¼‰ï¼Œæ­¥é•¿ä¸ºstride1
        self.branch2a = ConvNormLayer(ch_in, width, 1, stride1, act=act)
        # æ®‹å·®è·¯å¾„çš„3x3å·ç§¯ï¼Œæ­¥é•¿ä¸ºstride2
        self.branch2b = ConvNormLayer(width, width, 3, stride2, act=act)
        # æ®‹å·®è·¯å¾„çš„ç¬¬äºŒä¸ª1x1å·ç§¯ï¼ˆå‡ç»´ï¼‰ï¼Œæ­¥é•¿ä¸º1ï¼Œä¸å¸¦æ¿€æ´»å‡½æ•°
        self.branch2c = ConvNormLayer(width, ch_out * self.expansion, 1, 1)

        self.shortcut = shortcut
        # å¦‚æœä¸ä½¿ç”¨æ’ç­‰shortcut
        if not shortcut:
            # ResNet-Då˜ä½“ï¼šå¦‚æœéœ€è¦ä¸‹é‡‡æ ·(stride=2)ï¼Œshortcutè·¯å¾„ä½¿ç”¨å¹³å‡æ± åŒ–+1x1å·ç§¯ï¼ˆè¾“å‡ºé€šé“è°ƒæ•´ä¸ºexpansionå€ï¼‰
            if variant == 'd' and stride == 2:
                self.short = nn.Sequential(OrderedDict([
                    ('pool', nn.AvgPool2d(2, 2, 0, ceil_mode=True)),
                    ('conv', ConvNormLayer(ch_in, ch_out * self.expansion, 1, 1))
                ]))
            # å…¶ä»–æƒ…å†µæˆ–éResNet-Då˜ä½“ï¼šshortcutè·¯å¾„ä½¿ç”¨æ­¥é•¿ä¸ºstrideçš„1x1å·ç§¯ï¼ˆè¾“å‡ºé€šé“è°ƒæ•´ä¸ºexpansionå€ï¼‰
            else:
                self.short = ConvNormLayer(ch_in, ch_out * self.expansion, 1, stride)

        # è·å–æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°
        self.act = nn.Identity() if act is None else get_activation(act) 

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        """
        # è®¡ç®—æ®‹å·®è·¯å¾„è¾“å‡º
        out = self.branch2a(x)
        out = self.branch2b(out)
        out = self.branch2c(out)

        # è·å–shortcutè·¯å¾„è¾“å‡º
        if self.shortcut:
            # å¦‚æœä½¿ç”¨æ’ç­‰shortcutï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥x
            short = x
        else:
            # å¦åˆ™ï¼Œé€šè¿‡å®šä¹‰çš„shortæ¨¡å—å¤„ç†è¾“å…¥x
            short = self.short(x)

        # å°†æ®‹å·®è·¯å¾„å’Œshortcutè·¯å¾„çš„è¾“å‡ºç›¸åŠ 
        out = out + short
        # åº”ç”¨æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°
        out = self.act(out)

        return out


class Blocks(nn.Module):
    """
    Blocksç±»ï¼šç”¨äºæ„å»ºç”±å¤šä¸ªç›¸åŒç±»å‹çš„æ¨¡å—ç»„æˆçš„åºåˆ—ç½‘ç»œå—ç»„
    è¿™ä¸ªç±»åœ¨ç¥ç»ç½‘ç»œä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œç”¨äºåˆ›å»ºé‡å¤çš„ç½‘ç»œå—åºåˆ—ï¼Œæ˜¯æ„å»ºå¤æ‚ç½‘ç»œç»“æ„çš„åŸºç¡€ç»„ä»¶
    """
    def __init__(self, ch_in, ch_out, block, count, stage_num, act='relu', input_resolution=None, sr_ratio=None, kernel_size=None, kan_name=None, variant='d'):
        """
        åˆå§‹åŒ–Blocksæ¨¡å—
        
        å‚æ•°:
            ch_in (int): è¾“å…¥é€šé“æ•°
            ch_out (int): è¾“å‡ºé€šé“æ•°
            block (nn.Module): è¦é‡å¤ä½¿ç”¨çš„åŸºç¡€æ¨¡å—ç±»å‹ï¼Œå¦‚CSPLayerã€C3ç­‰
            count (int): è¦å †å çš„æ¨¡å—æ•°é‡
            stage_num (int): å½“å‰æ‰€å¤„çš„ç½‘ç»œé˜¶æ®µç¼–å·ï¼Œå½±å“ç¬¬ä¸€ä¸ªå—çš„æ­¥é•¿è®¾ç½®
            act (str): æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤ä¸º'relu'
            input_resolution (tuple): è¾“å…¥ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ŒæŸäº›æ³¨æ„åŠ›æœºåˆ¶éœ€è¦
            sr_ratio (int): ç©ºé—´ç¼©å‡æ¯”ç‡ï¼Œç”¨äºæ³¨æ„åŠ›æœºåˆ¶
            kernel_size (int): å·ç§¯æ ¸å¤§å°
            kan_name (str): å¯èƒ½ç”¨äºæ ¸æ³¨æ„åŠ›ç½‘ç»œçš„åç§°æŒ‡å®š
            variant (str): æ¨¡å—å˜ä½“ç±»å‹ï¼Œé»˜è®¤ä¸º'd'ï¼Œå½±å“ç½‘ç»œç»“æ„ç»†èŠ‚
        """
        super().__init__()

        # åˆ›å»ºä¸€ä¸ªæ¨¡å—åˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾æ‰€æœ‰çš„blockå®ä¾‹
        self.blocks = nn.ModuleList()
        
        # å¾ªç¯åˆ›å»ºcountä¸ªblockå®ä¾‹
        for i in range(count):
            # æ ¹æ®ä¸åŒçš„å‚æ•°ç»„åˆï¼Œåˆ›å»ºä¸åŒç±»å‹çš„block
            
            # æƒ…å†µ1: å¦‚æœæä¾›äº†input_resolutionå’Œsr_ratioï¼Œç”¨äºåˆ›å»ºå¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„å—
            if input_resolution is not None and sr_ratio is not None:
                self.blocks.append(
                    block(
                        ch_in, 
                        ch_out,
                        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå—ä¸”ä¸æ˜¯ç¬¬2é˜¶æ®µï¼Œåˆ™ä½¿ç”¨æ­¥é•¿2è¿›è¡Œä¸‹é‡‡æ ·ï¼›å¦åˆ™ä½¿ç”¨æ­¥é•¿1
                        stride=2 if i == 0 and stage_num != 2 else 1, 
                        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå—åˆ™ä¸ä½¿ç”¨shortcutè¿æ¥ï¼›å¦åˆ™ä½¿ç”¨
                        shortcut=False if i == 0 else True,
                        variant=variant,
                        act=act,
                        input_resolution=input_resolution,
                        sr_ratio=sr_ratio)
                )
            # æƒ…å†µ2: å¦‚æœæä¾›äº†kernel_sizeï¼Œåˆ›å»ºæŒ‡å®šå·ç§¯æ ¸å¤§å°çš„å—
            elif kernel_size is not None:
                self.blocks.append(
                    block(
                        ch_in, 
                        ch_out,
                        stride=2 if i == 0 and stage_num != 2 else 1, 
                        shortcut=False if i == 0 else True,
                        variant=variant,
                        act=act,
                        kernel_size=kernel_size)
                )
            # æƒ…å†µ3: å¦‚æœæä¾›äº†kan_nameï¼Œåˆ›å»ºå¸¦æœ‰æ ¸æ³¨æ„åŠ›ç½‘ç»œçš„å—
            elif kan_name is not None:
                self.blocks.append(
                    block(
                        ch_in, 
                        ch_out,
                        stride=2 if i == 0 and stage_num != 2 else 1, 
                        shortcut=False if i == 0 else True,
                        variant=variant,
                        act=act,
                        kan_name=kan_name)
                )
            # æƒ…å†µ4: é»˜è®¤æƒ…å†µï¼Œåˆ›å»ºåŸºæœ¬çš„block
            else:
                self.blocks.append(
                    block(
                        ch_in, 
                        ch_out,
                        stride=2 if i == 0 and stage_num != 2 else 1, 
                        shortcut=False if i == 0 else True,
                        variant=variant,
                        act=act)
                )
            
            # ç¬¬ä¸€ä¸ªå—ä¹‹åï¼Œæ›´æ–°è¾“å…¥é€šé“æ•°ä¸ºè¾“å‡ºé€šé“æ•°ä¹˜ä»¥blockçš„æ‰©å±•ç³»æ•°
            # è¿™æ˜¯å› ä¸ºè®¸å¤šå—ï¼ˆå¦‚ResNetä¸­çš„Bottleneckï¼‰åœ¨å†…éƒ¨ä¼šæ‰©å±•é€šé“æ•°
            if i == 0:
                ch_in = ch_out * block.expansion

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        
        æŒ‰é¡ºåºé€šè¿‡æ‰€æœ‰blockså¤„ç†è¾“å…¥ç‰¹å¾
        
        å‚æ•°:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾
            
        è¿”å›:
            torch.Tensor: ç»è¿‡æ‰€æœ‰blockå¤„ç†åçš„ç‰¹å¾å›¾
        """
        out = x
        # ä¾æ¬¡é€šè¿‡æ¯ä¸ªblockè¿›è¡Œç‰¹å¾æå–
        for block in self.blocks:
            out = block(out)
        return out